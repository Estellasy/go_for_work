# kv存储
1. 这个项目有什么优点
- 写性能优秀：对于所有写入，都是顺序写，减少磁盘的寻道时间，实现高吞吐量；
- 读性能尚可：读操作只需要进行一次磁盘IO，且大概率是随机IO。通过对文件系统提供的缓存进行一些优化，读过的数据会加速访问；
- 支持过期时间：在内存中维护一个map，记录每个key和其对应的过期时间，过期时间信息会持久化到一个特定文件中，启动数据库时会加载到内存；

2. 介绍一下bitcask模型
bitcask是一个日志形式存储、顺序写的模型，像是简化版的LSM树模型。

一个batcask实例就是系统上的一个文件夹，数据文件在磁盘上顺序写入，文件大小达到限制后，会将其封存，变成只读文件。然后打开新文件进行写入操作。每次只有一个文件能够进行写入，加过active文件。

数据文件中的每一个记录成为一个entry，包括记录类型、batchid、key size、value size、expire、key、value等。

更新和删除操作也是追加一条记录到数据文件中。

在内存中，维护索引，bitcask论文中采用的是哈希表，每次操作先写文件，后更新内存。但是我用的是B树。

读取操作直接读内存的索引，索引记录了目标所在的文件位置，然后去磁盘中进行查找读取。

数据文件值写入不删除，所以会有很多冗余，bitcask有merge操作，即遍历所有数据文件，只保留有效记录。

因为日志型的数据文件本身是无序的，所以查询必须依靠内存索引进行，在启动数据库时需加载索引信息。

bitcask使得可以装下数倍于内存的数据，因为内存只需要保存key即可。

3. 介绍一下LSM树模型

LSM树全称为结构化合并树，其设计目标是提供比传统B+树更好的写性能。

LSM树将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，由此提升了写性能，是一种基于硬盘的数据结构。

在LSM树中，将磁盘的随机写转换为顺序写来提高写性能，而付出的代价是牺牲部分读性能和写放大的问题。

磁盘中会定期进行merge操作，优化读性能。LSM有效地规避了磁盘随机写入问题，但是读取时可能需要访问较多的磁盘文件。

4. 做项目时有没有自己的想法

如何组织这些数据。当有大量写时，直接覆盖比每次查询更快。在查询数据时，先查询当前的log中有没有，再查询索引。如果索引中存在，将其调入内存。下一次查询时直接从内存中读取。内存淘汰策略采用的是LRU。

5. merge操作如何进行
- 遍历所有数据文件，只保留写记录。
- 将所有有效的entry放入一个新的entry数组后，讲这些有效entry重新写入一批数据文件中，并更新索引。

## 内存和磁盘设计
在WAL（Write Ahead Log）中，通常将数据存储在Segment中，这些段文件有一个或者多个block组成，而每个block又包含多个数据块chunk。

1. **Segment（段）**：
   - 段是 WAL 中数据的物理单位，通常表示为一个单独的文件。
   - 每个段文件是以追加方式写入的，意味着一旦数据被写入到段中，它就不会被修改或删除。
   - 段文件可以包含一个或多个块，取决于写入的数据量和段的大小限制。
2. **Block（块）**：
   - 块是段文件中的逻辑单位，用于组织数据的存储。
   - 每个块的大小通常是固定的，以便更有效地管理和存储数据。
   - 在 WAL 中，块的大小通常是 32KB。块可以看作是段文件中的子集，它将数据分成更小的单元以便于管理。
3. **Chunk（数据块）**：
   - 数据块是块中的逻辑单位，用于存储实际的数据。
   - 每个数据块包含一个或多个数据项，它们通常是记录、事务或其他类型的数据。
   - 数据块通常具有自己的头部信息，用于标识数据类型、长度等元数据。

关系：
- 一个段（segment）文件可以由一个或多个块（block）组成，每个块具有固定的大小，例如 32KB。
- 每个块（block）内部存储着一个或多个数据块（chunk），这些数据块是实际存储数据的逻辑单位。
- 因此，数据从应用程序写入到数据块（chunk）中，然后数据块被组织成块（block），最终块被追加到段（segment）文件中。

在 Write-Ahead Log（WAL）中，通常将数据存储在段（segment）文件中。这些段文件由一个或多个块（block）组成，而每个块都包含一个或多个数据块（chunk）。

## 写入操作

我们只检查block是否已满，而没有检查segment是否已满，原因是在写数据时，通常以块为单位写入，而不是一次性写入整个段。

考虑到 Write Ahead Log (WAL) 的特性，写入是以块为单位进行的，每个块的大小通常是固定的。在实际场景中，如果需要写入的数据超过了一个块的大小，我们需要将其分成多个块进行写入。

而判断块是否已满则是为了在写入数据时，及时进行块的切换，以避免数据的丢失。如果当前块已满，我们需要将数据写入到下一个块中。因此，在这段代码中，我们只需要判断当前块是否已满，而不需要判断整个段是否已满。

当然，在实际的应用场景中，可能还会对段的大小进行控制，以确保 WAL 文件不会无限增长。在这种情况下，可以在写入数据时，结合块的大小和段的大小进行判断，以避免段文件无限增长造成的问题。

在上面提到的 `write` 方法中，确实存在这样的逻辑：

1. 首先，将数据写入到缓冲区中。在 `write` 方法中，调用了 `writeToBuffer` 方法，该方法将数据写入到缓冲区 `chunkBuffer` 中。
    
2. 在缓冲区中进行块处理。在 `writeToBuffer` 方法中，根据当前块的大小和数据的大小，决定是否在当前块中写入数据，或者将数据分成多个块进行写入。这个过程主要是逻辑上的处理，确保数据被正确地写入到块中。
    
3. 最后，将缓冲区中的内容写入到段文件中。在 `write` 方法中，调用了 `writeChunkBuffer` 方法，该方法将缓冲区中的数据直接写入到段文件的底层文件中。

在这个过程中，块的概念主要是**逻辑**上的存在，用于将数据进行**分块处理**，并决定数据在缓冲区中的存储方式。实际上，缓冲区中的数据被写入到段文件中后，会**按照文件系统的逻辑划分为文件系统中的块**。（主要是便于寻址，根据block大小+offset快速定位）

`readInternal` 方法用于从段文件中读取数据块，并返回读取的数据、下一个数据块的位置信息以及可能出现的错误。下面是对该方法的总结：

1. 方法签名：`func (seg *segment) readInternal(blockNumber uint32, chunkOffset int64) ([]byte, *ChunkPosition, error)`

2. 参数：
    - `blockNumber`：要读取的数据块的编号。
    - `chunkOffset`：数据块内的偏移量。

3. 返回值：
    - `[]byte`：读取的数据块内容。
    - `*ChunkPosition`：下一个数据块的位置信息。
    - `error`：可能出现的错误，如文件已关闭、IO 错误等。

4. 实现逻辑：
    - 首先检查段文件**是否已关闭**，如果已关闭则返回错误。
    - 创建变量保存结果、块数据和头部信息，并获取段文件的大小。
    - 循环读取数据块，直到达到文件末尾。
    - 在循环中，尝试从缓存中读取数据块，如果缓存命中则直接使用缓存中的数据块，否则从段文件中读取数据块。
    - 根据数据块的类型更新下一个数据块的位置信息：
	    - 如果数据块是完整的或者是最后一个数据块，更新下一个数据块的块编号和偏移量。
	    - 如果是最后一个数据块，并且当前数据块的填充区已经被数据占满，说明下一个数据块应该在下一个块中，更新块编号为当前块编号加一，偏移量为零。
		- 如果数据块类型是中间类型，则继续读取下一个数据块。
	    - 读取数据块后，校验头部信息的校验和，并根据数据块类型更新下一个数据块的位置信息。
	    - 将读取的数据追加到结果中，并返回结果、下一个数据块的位置信息以及可能的错误。

注意事项：
  - 方法内部使用了同步机制来确保多个 goroutine 安全地访问段文件和缓存。
  - 通过缓存机制提高了读取性能，避免了频繁地从文件中读取数据块。
  - 通过校验和检查确保读取的数据块的完整性。

## 内存设计
Index索引，需要一种支持高效插入、读取、删除数据的结构，如果需要数据高效遍历，需要选择**天然有序**的数据结构。

选择google/btree实现索引


## 批处理相关
**数据结构**
```go
type Batch struct {
	db *DB // 当前数据库实例指针
	pendingWrites []*LogRecord // save the data to be written 写入数据库日志记录
	pendingWritesMap map[uint64][]int // map record hash key to index, fast lookup to pendingWrites 快速查找 存储key
	options BatchOptions // 批处理选项（同步、只读）
	mu sync.RWMutex
	committed bool // whether the batch has been committed
	rollbacked bool // whether the batch has been rollbacked
	batchId *snowflake.Node // 生成批处理唯一标识符
	buffers []*bytebufferpool.ByteBuffer // 字节缓冲池
}
```

Batch提供了
- Put
- PutWithTTL
等接口，都是**先写pendingWrites**，**先在pendingWrites中查找**，再查找Index。对于Delete操作，只查找pendingWrites，如果存在，则当即撤销记录，如果不存在，则添加一条记录到pendingWrites。等到commit一起删除。
如果expired，则删除index的对应记录。

在提交（Commit）操作中，对于记录（record）的类型（Type）不同，处理方式也会不同：
1. 如果记录的类型为删除（LogRecordDeleted），则会将其写入 WAL 文件中。即使记录已被删除，也需要将其写入 WAL 文件，以确保 WAL 文件中的操作与实际数据一致性。
2. 如果记录的类型为其他类型（例如新增、更新等），同样会将其写入 WAL 文件中。这样可以保证 WAL 文件记录了所有数据库的操作，以便在需要时进行恢复和重放。

总的来说，无论记录的类型是什么，都会将其写入 WAL 文件中，以确保操作的原子性和持久性。


## Watch
Watcher（观察者）：Watcher 是一个用于临时存储数据库修改事件（Event）信息的结构。它提供了方法来存储和获取事件，以及将事件发送给数据库的观察者。
```go
type Watcher struct {
	queue eventQueue
	mu    sync.RWMutex
}
```
Event（事件）：Event 是数据库被修改时产生的事件。它包含了键值对的变化信息，以及相应的操作类型（Put 或 Delete）和批次 ID（BatchID）等信息。
```go
type Event struct {
	Action  WatchActionType
	Key     []byte
	Value   []byte
	BatchId uint64
}
```

eventQueue（事件队列）：eventQueue 是用于存储 Event 对象的队列结构。它使用循环数组实现，并提供了方法来向队列中添加事件、从队列中取出事件等操作。
```go
// eventQueue 循环事件队列
type eventQueue struct {
	Events   []*Event
	Capacity uint64
	Front    uint64 // read point
	Back     uint64 // write point
}
```

**为什么需要watch操作**
观察者模式（Watch 操作）用于实现数据库的事件监听和通知机制。当数据库发生变化时（例如插入、更新、删除操作），将产生相应的事件，并由观察者（Watcher）来监听和处理这些事件。Watch 操作的主要作用是：

**实现数据库的异步事件通知：** 当数据库发生变化时，可以通过观察者模式异步通知相关的监听者，而不需要监听者轮询数据库的状态。

**提供实时性的数据更新：** 观察者可以实时获取数据库的变化情况，从而及时地对数据进行处理和响应。

**解耦数据库操作和业务逻辑：** 观察者模式将数据库操作和业务逻辑解耦，使得数据库操作和事件处理逻辑分离，提高了代码的可维护性和扩展性。


# Merge
Bitcask 是一种键值存储引擎，它将数据以日志的形式追加到日志文件中，并维护一个内存中的索引来加速读取操作。虽然 Bitcask 的追加写入操作非常高效，但是**由于不断追加数据，会导致日志文件不断增长**，可能会占用大量磁盘空间。

Merge 操作的主要目的是在保持数据持久性的同时，对磁盘上的数据**进行整理和压缩**，以减少磁盘空间的占用。具体来说，Merge 操作通常会做以下几件事情：

1. **合并数据文件**：将多个日志文件合并成一个更大的文件，以减少文件数量，提高读取效率。
2. **压缩数据**：将相同键的多个版本合并成一个版本，减少重复数据，节省磁盘空间。
3. **删除过期数据**：删除过期的数据，以释放磁盘空间。
4. **重建索引**：根据合并后的数据文件重新构建索引，以提高读取效率。

**Merge操作的流程**
1. **获取当前活跃的段文件 ID（prevActiveSegId）：** 在开始合并操作之前，首先获取当前数据库中活跃的段文件的 ID。
2. **切换写前日志（WAL）并创建新的活跃段文件：** 调用 `OpenNewActiveSegment()` 方法切换写前日志，并创建一个新的活跃段文件。这样，所有旧的段文件将会被合并到新的活跃段文件中。
3. **解锁数据库的互斥锁：** 在切换写前日志并创建新的活跃段文件后，解锁数据库的互斥锁。这是因为合并操作只会读取旧的段文件，而不会影响到当前的写入操作。
4. **打开新的合并数据库实例（mergeDB）：** 调用 `openMergeDB()` 方法打开一个新的数据库实例，用于将合并后的数据写入新的数据文件中。如果合并目录已经存在，则会先删除旧的合并目录。
4. **打开新的合并数据库实例（mergeDB）：** 调用 `openMergeDB()` 方法打开一个新的数据库实例，用于将合并后的数据写入新的数据文件中。如果合并目录已经存在，则会先删除旧的合并目录。
5. **遍历旧的数据文件并写入新的数据文件：** 遍历所有旧的数据文件，并将其中有效的数据写入到新的数据文件中。对于每条记录，会检查其在索引中的位置，并确保只处理索引中最新的记录。
6. **标记合并操作完成：** 在重写所有数据后，创建一个文件以指示合并操作已完成。这样，在数据库重新启动时，可以通过检查此文件是否存在来确定合并操作是否已完成。如果文件存在，则表示合并操作已完成，否则将删除合并目录并重新执行合并操作。
7. **关闭合并数据库实例：** 在合并操作完成后，关闭合并数据库实例以释放资源。
8. **重新打开原始数据库文件并重建索引（可选）：** 如果指定了 `reopenAfterDone` 参数为 `true`，则会重新打开原始数据库文件，并在合并操作完成后重新构建索引。

## 数据库
**DB数据结构定义**
```go
type DB struct {
	dataFiles        *wal.WAL // data files are a sets of segment files in WAL.
	hintFile         *wal.WAL // hint file is used to store the key and the position for fast startup.
	index            index.Indexer
	options          Options
	fileLock         *flock.Flock // 文件锁
	mu               sync.RWMutex
	closed           bool
	mergeRunning     uint32    // indicate if the database is merging
	batchPool        sync.Pool // 批处理对象池
	recordPool       sync.Pool // 记录池
	encodeHeader     []byte
	watchCh          chan *Event // user consume channel for watch events
	watcher          *Watcher
	expiredCursorKey []byte     // the location to which DeleteExpiredKeys executes.
	cronScheduler    *cron.Cron // cron scheduler for auto merge task 合并定时器
}
```

**fileFlock**
```go
// create file lock, prevent multiple processes from using the same database directory
fileLock := flock.New(filepath.Join(options.DirPath, fileLockName))
hold, err := fileLock.TryLock()
if err != nil {
	return nil, err
}
if !hold {
	return nil, ErrDatabaseIsUsing
}
```
`fileLock` 的作用是确保同一时间**只有一个进程可以访问数据库目录**。这是因为在文件系统中，同一时间只能有一个进程对同一个文件进行写操作，而对数据库文件的写操作需要确保原子性，以避免数据损坏或不一致性。因此，使用文件锁可以有效地防止多个进程同时对数据库进行写操作，从而保证了数据库的完整性和稳定性。

具体来说，当一个进程打开数据库时，它会**尝试获取文件锁**。如果成功获取锁，则说明当前没有其他进程在使用数据库目录，该进程可以**安全地打开数据库并进行操作**。如果获取锁失败，则说明已经有其他进程在使用数据库目录，当前进程将无法打开数据库，从而避免了多个进程同时对数据库进行写操作的情况发生。

**expiredCursorKey**

**cronScheduler** 自动合并定时器
```go
if len(options.AutoMergeCronExpr) > 0 {
	db.cronScheduler = cron.New(
		cron.WithParser(
			cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour |
				cron.Dom | cron.Month | cron.Dow | cron.Descriptor),
		),
	)
	_, err = db.cronScheduler.AddFunc(options.AutoMergeCronExpr, func() {
		// maybe we should deal with different errors with different logic, but a background task can't omit its error.
		// after auto merge, we should close and reopen the db.
		_ = db.Merge(true)
	})
	if err != nil {
		return nil, err
	}
	db.cronScheduler.Start()
}
```
`cronScheduler` 被用于启用自动合并任务。使用了第三方库 `cron`，该库提供了用于调度周期性任务的功能。

具体来说，`cronScheduler` 的作用是根据用户提供的定时表达式（`options.AutoMergeCronExpr`）来定期执行自动合并操作。在 `Open` 函数中，如果用户提供了定时表达式，就会创建一个 `cronScheduler` 实例，并**通过 `AddFunc` 方法向其添加一个定时任务，该任务会定期执行数据库的自动合并操作（`db.Merge(true)`）**。

当数据库被打开后，`cronScheduler` 就会开始根据定时表达式定期触发自动合并操作。这样就可以定期执行合并操作，以维护数据库的性能和稳定性，同时也可以确保数据文件不会无限增长，避免影响数据库的性能。

- 这个项目有什么优点？
	- 读写低延迟：利用Bitcask存储模型的文件追加写入特性，充分利用顺序IO优势。
	- 高吞吐量：写入时不需要在磁盘上排序，Bitcask的日志结构文件设计在写入过程中减少了磁盘磁头的移动。
	- 能够处理大于内存的数据集：RoseDB的数据访问设计对内存中的索引数据结构进行查找，使得数据集非常大，查找数据也非常高效。
	- 一次磁盘IO可以获取任意键值对：内存索引数据结构直接指向数据所在磁盘位置，不需要多次寻址来读取一个值。归功于操作系统文件缓存以及WAL的block缓存。
	- 性能快速稳定：写入操作最多需要一次打开当前文件的尾部寻址，然后进行追加写入，写入后更新内存。整个流程不会受到数据库数据量大小的影响，性能稳定。
	- 崩溃恢复快速：db文件只追加写入一次，恢复操作需要检查记录并验证CRC数据，以确保数据一致性。
	- 备份简单：只追加写入一次磁盘格式简化了过程，任何按照磁盘顺序归档或复制文件的工具都将正确备份或复制到db数据库。
	- 批处理操作：保证原子性、一致性、持久性。（为什么不保证隔离性？）批处理操作的新写入操作在提交之前被缓存到内存中，如果批处理成功提交，则批处理所有写入操作都将持久化到磁盘，如果批处理失败，所有写入操作均会被丢弃。也就是要么全部成功，要么全部失败。


- 介绍一下bitcask模型？
	- 一个bitcask示例就是系统上的一个目录，限制同一时刻只有一个进程能够打开目录，同一时刻只有一个活跃的文件用于写入新数据。
	- 当前活动文件写满后，就会被关闭，只能用于读取。
	- 删除数据追加新的记录标识记录已被删除，下次merge的时候才会将无效数据清理掉。
		数据文件是多条数据集合的排列。
		磁盘文件写完后，会更新内存中的数据结构，叫做keydir，也就是全部key的集合，存储内容为key到一条磁盘文件数据的位置。
		读取数据：先根据key在内存中找到对应的记录（chunk），这个记录记录的是磁盘中的位置（pos），然后根据这个位置找到磁盘上对应的数据文件，经过解析得到完整数据。
		删除并不是删除数据，而是新增一条被标识删除的记录。
		merge方法用于清理所有不可变的数据文件，
		- merge操作流程
			- 遍历所有不可变的旧数据文件
			- 将有效数据重新写到新的数据文件，并将旧数据文件删掉
			- hintfile：索引文件，启动数据库时可以直接加载（加速），而不用全部加载数据文件。（hint不存储value，比数据文件小）

    bitcask提供的一些面向用户的API操作接口
		- Open
		- Get
		- Put
		- Delete
		- list_keys
		- Fold
		- Merge
		- Sync 缓冲区持久化
		- Close

### 你做的这个项目能简单介绍一下吗

我开发的项目是一个基于 Bitcask 存储模型的 KV 数据库。bitcask 是一种高性能的持久化存储引擎，其基本原理是采用了预写日志的数据存储方式，每一条数据在写入时首先会追加写入到数据文件中，然后更新内存索引，内存索引存储了 Key 和 Value 在磁盘上的位置，读取数据时，会先从内存中根据 key 找到对应 Value 的位置，然后再从磁盘中取出实际的 Value。

基于这种模型，其读写性能都非常高效快速，因为每次写入实际上都是一次顺序 IO 操作，然后更新内存，每次读取也是直接从内存中找到对应数据在磁盘上的位置。

### 为什么会做这个
对数据库存储系统实现的好奇心，看到了对应的 Bitcask 的论文，想要自己去实现

### 有哪些使用场景
**缓存系统**：KV 数据库可用作缓存系统的后端存储，以提供快速的数据访问和响应能力。由于 Bitcask 存储模型具有高性能和低读写放大的特性，它适合存储频繁访问的热数据，提供快速的缓存读取操作。

**日志存储**：KV 数据库可以作为日志存储系统使用，将日志数据持久化到磁盘上的日志文件中。Bitcask 存储模型的追加写入方式使得日志的写入操作非常高效，确保了日志的可靠存储和后续分析。

**Key 小 Value 大的 KV 数据存储**：Bitcask 将 key 和对应的索引都维护在了内存当中，这样如果 key 较小的话，那么内存当中能够维护的数据量就更多，并且 Value 是在磁盘上存储的，因此可利用磁盘更大的空间来存储 Value。

**缺点**

- **所有的 key 必须在内存中维护**
始终将所有 key 保留在内存中，这意味着您的系统必须具有足够的内存来容纳所有的 key。

- **启动速度受数据量的影响**
数据库启动时，会加载所有的数据，并且会重放所有的操作，以此来构建内存索引，如果数据量较大，这个过程可能会非常漫长

### 磁盘上产生了无效的数据，如何清理
实现了 Bitcask 论文中提到的 Merge 方案，Merge 实际上就是对磁盘数据空间进行清理的操作，其基本执行流程是遍历所有的数据，并将有效的数据进行重写，然后使用新的文件替换旧的文件，以此达到回收空间的效果。

- 不会，Merge 实际上是在新的目录打开了新的 Bitcask 进程实例，这个实例和原目录上运行的实例互不冲突，Merge 的时候，只会读取原 Bitcask 实例的索引数据结构，判断数据是否有效，并不会对原来的实例产生任何影响，并且原实例的写入会写到新的文件中，不会参与到 Merge 过程中，所以对写入也没有影响。

**追问：**

- **Merge 的过程会阻塞读写操作吗**
- 不会，Merge 实际上是在新的目录打开了新的 Bitcask 进程实例，**这个实例和原目录上运行的实例互不冲突**，Merge 的时候，**只会读取原 Bitcask 实例的索引数据结构**，判断数据是否有效，并不会对原来的实例产生任何影响，并且原实例的写入会写到新的文件中，不会参与到 Merge 过程中，所以对写入也没有影响。
    
- **Merge 过程万一很漫长，中途挂了怎么办**
- 在具体实现中，会在 Merge 结束之后，在磁盘文件中**写入一个 Merge 完成的标识**，只有当有这个标识的时候，我们才认为一次 Merge 是完整的，否则 Merge 都是不完整的，直接删除掉 Merge 的数据目录即可。

### 写操作是如何保证原子性的
采用了预写日志的方式，和其他大多数系统一样，WAL 通常是保证事务原子性和持久性的关键，在 Bitcask 存储模型中，比较特殊的是 WAL 文件本身就是数据文件，所以天然可以保证原子性，我们在写入的时候加上了一个完成的标识，并且给每一批次的数据都附了一个全局递增的 id，只有全部提交完成了，这个批次的数据才算完成，否则都不会进行加载。


### 和 LevelDB、BoltDB、Redis 的区别

LevelDB 是经典的 LSM Tree 存储模型，其基本架构大致分为了 WAL、memtable、SSTable，数据写入首先会到 wal 中保证持久化，然后更新到内存的 memtable 中，如果 memtable 满了，则 flush 到磁盘的 sstable 中。

读数据会从 memtable 查找，如果没找到，则从磁盘上的多级 sstable 中查找，读性能不稳定。

BoltDB 是 B+ 树存储模型，读性能稳定，但是写入是随机 IO，性能较差。

Redis 是一种纯内存的数据结构服务，也可以持久化到磁盘中，但其实际上是一种面向内存的 KV 存储，数据量受到内存容量的影响。

而 Bitcask 存储模型，写性能和 LSM 模型相当，读性能也很稳定，读写都是一次磁盘 IO 操作即完成，并且相较于 Redis，Value 是不会存储到内存中，节省了内存空间，并且性能能够和 Redis 维持在一个数量级。
    

### 做了哪些改进

- **内存索引限制**
- 前面说到，Bitcask 的一大缺点就是所有的 key 都必须在内存中维护，这样数据库中能存储的数据量就受到了内存容量的限制，而我的项目中，创造性的**使用了持久化的 B+ 树**来作为索引数据结构，这样就可以将索引存储到磁盘上，突破了内存容量的限制。但是这样带来的一个副作用便是**读性能会下降**，因为原来是直接从内存中就能到获取到 Value 的位置，但是使用 B+ 树存储索引的话，还需**要从磁盘 B+ 树中获取索引，然后再到磁盘中获取 Value，相当于多了几次磁盘 IO 操作**
    

- **内存索引锁粒度优化**
- 在我的最初的项目中，内存索引是单个数据结构，并且为了保证并发安全，**这个结构的读写都需要加锁**，如果在大数据量下，所有的数据都会竞争这把锁，所以我将索引进行了分区，并**通过哈希函数将 key 映射到不同的索引结构当中**，大大减少了并发冲突
    

- **启动速度优化**
- 为了避免在重启的时候全量加载所有的数据来构建内存索引，我的项目中实现了 Bitcask 论文中提到的 Hint 文件，Hint 文件实际上就是一个 **key+索引**的文件，它**不存储 Value，相较于原始文件容量会小很多**，这样重启加载的时候，直接加载这个 Hint 文件
    
- 追问 1：Hint 文件是在什么时候生成的呢
- Merge 的时候生成的，Merge 时实际上所有的数据都是有效的，这个时候只需要依次存储对应的 key 和索引数据
    
- 追问 2：Hint 文件的格式是什么
- 和数据文件是一样的，都采用了日志追加的方式


- **持久化策略优化**
- 在最开始的设计中，默认的刷盘策略是交给了操作系统来调度，这样的好处是性能很好，但是存在丢失数据的风险，在实际环境中，我们可以再提供另一个选项，用户可以设置积累到多少个字节之后，再进行一次持久化。这相当于提供了一个折中的方案，相较于之前的要么每条都持久化，要么完全交给操作系统，这样能够让用户自己灵活配置。
    

- **数据文件布局优化**
- 我还参考了 LevelDB 和 RocksDB 的 WAL 文件的格式，将数据文件的组织形式改为 block（32KB） 的方式，加速数据读写的效率。

# 云存储
## 设计一个云存储系统，要考虑什么因素？
1. 从我做的这个来看，主要逻辑是用户上传文件，本地先保存副本，然后本地上传到对接的对象存储。在本地上传到云的过程中考虑分片上传、秒传、断点续传等，用对接的对象存储来保证数据的一致性，本地做了一些身份认证和授权、元数据管理、数据上传流程的工作。
2. 主要考虑的因素有：
   1. 多个对象存储的兼容和集成：需要统一不同服务的api调用方式
   2. 数据管理：元数据管理，管理和存储文件的元数据，确保其在不同存储服务之间的一致性，还没有做多云环境的同步和复制
   3. 安全性：身份认证和授权，确保用户只能访问有权限的数据
   4. 数据加密：数据传输和存储过程中，确保数据隐私和安全
   5. 缓存：提高数据访问速度
   6. 故障恢复和容错：交给对象存储完成
   7. 成本优化：冷存储和热存储，不同的存储层，交给对象存储完成
   8. 易用性：日志和监控

## JWT详解
1. Session
Session的使用过程是怎样的？

- 用户进行登录时，提交包含用户名和密码的表大，放入HTTP请求报文中；
- 服务器进行验证，如果正确则将用户信息存储到Redis中，在Redis中的Key就是sessionID；
- 服务器返回的响应报文的首部包含这个SessionID，浏览器将其保存到Cookie中；
- 之后的每一次请求，都会将Cookie携带的SessionID发送给服务器，服务器根据SessionID查找Redis中的用户信息。

2. Token

Token的使用过程是怎样的？

- 客户端使用用户名和密码请求登录；
- 服务端收到请求，验证用户名和密码；
- 验证成功后，服务端会签发一个Token，再将Token返回给客户端；
- 客户端收到Token后将其存储起来，比如放入Cookie中；
- 客户端每次向服务端请求资源时需要携带这个Token，可以在Cookie或者Token中携带；
- 服务端收到请求，需要验证客户端请求中携带的Token；
- 验证成功后，服务端会从Token中获取用户的相关信息，并进行相关操作。


> Token对比Session的优点
> 
> **无状态**：Token是无状态的，而Session需要维护服务器的状态信息。这使得Token更容易在分布式系统和负载均衡器中进行管理；
>
> **扩展性**：由于Token的无状态，可以将Token轻松集成到API、移动应用程序等不同系统中；
>
> **安全性**：Token可以防止CSRF（跨站请求伪造）攻击，因为Token不包含用户名和密码，所以不能被这些攻击方式利用；
>
> **性能**：Session需要在服务端维护状态信息，可能会影响性能；Token可以在客户端进行验证和授权，减少了对服务器的负载；


3. 什么是JWT

JWT的全称是`Json Web Token`，本质是字符串，是Token的一种具体实现方式。

**JWT的认证流程**
- JWT生成
  - 前端通过Web表单将用户名和密码发送到后端的接口，这个过程一般是POST请求。可以通过SSL加密传输，防止敏感信息嗅探；
  - 后端核对用户名和密码成功后，**将包含用户信息的数据作为JWT的Payload**，与**JWT Header**分别进行Base64编码拼接后签名，形成一个JWT Token，形成的JWT Token就是一个形如`lll.zzz.xxx`的字符串；
  - 后端将JWT Token字符串作为登陆成功的结果返回给前端。前端可以将返回的结果保存在浏览器中，退出时删除保存的JWT Token即可。
- 请求
  - 前端在每次请求时将JWT Token放入HTTP请求头的`Authorization`属性中（解决XSS和XSRF问题）；
  - 后端检查前端传过来的JWT Token，验证有效性，比如检查签名是否正确、是否过期、token的接收方是否是自己等；
  - 验证通过后，后端解析出JWT Token包含的用户信息，进行其他逻辑操作，返回结果。

![](./Pasted%20image%2020240515222455.png)

4. 为什么使用JWT？

**传统Session的弊端**
- 服务端状态管理：需要在服务端维护用户的状态信息，可能会增加服务器的负担和复杂度；
- 分布式环境下的问题：跨节点Session；
- 扩展性差：Session跨域传递问题；
- CSRF攻击：Session认证容易受到CSRF攻击，攻击者可以通过伪造请求来获取用户的会话信息；
- 跨域问题：由于Session是基于Cookie实现的，而Cookie在跨域请求时会受到浏览器的同源策略限制，可能会导致某些跨域场景下无法正常使用。
- 占用带宽：每次请求都要携带Session ID等相关信息，增加了请求头的大小，占用了带宽，并且可能会影响网络性能。

**JWT的好处**
- 无状态
- 可拓展性：基于JSON
- 安全性：签名和加密
- 可定制性：可以根据业务需求自定义

**JWT的缺点**
- 信息泄露：base64编码不安全
- 不可撤销性：一经签发不可撤销
- 大小限制：网络传输和存储方面性能损失
- 安全更新：如果需要更新JWT中的某些信息，例如访问权限、过期时间等，需要重新签发一个新的JWT。但是，这可能会导致之前签发的JWT仍然有效，从而导致安全性问题。

5. JWT结构

JWT由三部分组成分别是Header、Payload、Signature，每个部分都使用base64编码，使用`.`连接，形成一个完成的JWT字符串。

- Header：描述JWT的元数据，包括类型（`JWT`）和使用的签名算法（`HS256`）。
- Payload：有效载荷部分，是JWT的主体内容，也是一个JSON对象。有7个字段可以选择。如发行人、到期时间、主题、用户、发布时间、JWT ID等，还可以定义一些私有字段；默认情况下JWT是未加密的，拿到JWT字符串后可以转换为原本的JSON数据，任何人都可以解读内容。JWT只适合在网络中传输一些非敏感信息。
- Signature：使用Header中的指定签名算法对编码后的header和payload数据进行签名，并且生成哈希。首先需要指定一个密钥，**存储在服务器中**，不能向用户公开。使用header中的签名算法生成签名，计算出签名哈希后，JWT头，有效载荷和签名哈希的三个部分组合成一个字符串，每个部分用.分隔，就构成整个JWT对象。

服务端如何解析：
- header和payload可以直接利用base64解码出原文，从header中获取哈希签名的算法，从payload中获取有效数据
- signature由于使用了不可逆的加密算法，无法解码出原文，它的作用是校验token有没有被篡改。服务端获取header中的加密算法之后，利用该算法加上secretKey对header、payload进行加密，比对加密后的数据和客户端发送过来的是否一致。注意secretKey只能保存在服务端，而且对于不同的加密算法其含义有所不同，一般对于MD5类型的摘要加密算法，secretKey实际上代表的是盐值。

6. JWT的种类
JWT（JSON Web Token）根据用途和属性的不同，可以分为以下几种：
- JWS：JSON Web Signature，用于验证消息的完整性和来源。JWS包含Header、Payload和Signature三个部分，其中Signature是通过指定算法生成的，用于验证消息是否被篡改。
- JWE：JSON Web Encryption，用于对消息进行加密。JWE包含Header、Payload和Encryption三个部分，其中Encryption使用指定算法对消息进行加密，以保证机密性。
- JWK：JSON Web Key，用于描述用于加密、解密或签名JWT的公钥和私钥信息。JWK通常包括key type、key id、public key等属性。
- JWA：JSON Web Algorithm，用于描述JWT所支持的加密、解密和签名算法，例如HMAC、RSA、AES等。
需要注意的是，这些JWT的种类并不是相互独立的，它们可以结合使用来实现更强大的功能，例如使用JWS和JWE来同时保证消息的完整性、机密性和来源可信度。

```go
package utils

import (
	"errors"
	"fmt"
	"time"

	"github.com/golang-jwt/jwt/v4"  // 调包
)

type MyClaims struct {
	jwt.RegisteredClaims
	ID    int64
	State string `json:"state"`
}

var accessSecret = []byte("go-zero-netdisk")  // 指定accessSecret
var refreshSecret = []byte("114514")          // 指定refreshSecret

// GetToken 获取 accessToken 和 refreshToken
func GetToken(id int64, state string) (string, string) {
	// 临时访问令牌
	accessToken := jwt.NewWithClaims(jwt.SigningMethodHS256, MyClaims{
		RegisteredClaims: jwt.RegisteredClaims{ // Payload部分
			Issuer:    "accessToken", // 发行人
			IssuedAt:  jwt.NewNumericDate(time.Now()),  // 签发时间
			ExpiresAt: jwt.NewNumericDate(time.Now().Add(3 * time.Hour)), // 过期时间
		},
		ID:    id,
		State: state,
	})

	// 刷新令牌
	refreshToken := jwt.NewWithClaims(jwt.SigningMethodHS256, MyClaims{
		RegisteredClaims: jwt.RegisteredClaims{
			Issuer:    "refreshToken",
			IssuedAt:  jwt.NewNumericDate(time.Now()),
			ExpiresAt: jwt.NewNumericDate(time.Now().Add(24 * 30 * time.Hour)),
		},
		ID:    id,
		State: state,
	})

	accessTokenSigned, err := accessToken.SignedString(accessSecret)   // 签发accessTokenSigned
	if err != nil {
		fmt.Println("获取Token失败，Secret错误")
		return "", ""
	}
	refreshTokenSigned, err := refreshToken.SignedString(refreshSecret)
	if err != nil {
		fmt.Println("获取Token失败，Secret错误")
		return "", ""
	}
  // 生成访问令牌和刷新令牌
	return accessTokenSigned, refreshTokenSigned
}

// ParseToken 解析token
func ParseToken(accessTokenString, refreshTokenString string) (*MyClaims, bool, error) {
	fmt.Println("Parsing Token...")
	// 解析访问令牌
	accessToken, err := jwt.ParseWithClaims(accessTokenString, &MyClaims{}, func(token *jwt.Token) (interface{}, error) {
		return accessSecret, nil
	})
	if err != nil {
		return nil, false, err
	}
	if claims, ok := accessToken.Claims.(*MyClaims); ok && accessToken.Valid {
		return claims, false, nil
	}

	fmt.Println("Refresh Token")
	refreshToken, err := jwt.ParseWithClaims(refreshTokenString, &MyClaims{}, func(token *jwt.Token) (interface{}, error) {
		return refreshSecret, nil
	})
	if err != nil {
		return nil, false, err
	}
	if claims, ok := refreshToken.Claims.(*MyClaims); ok && refreshToken.Valid {
		return claims, false, nil
	}

	return nil, false, errors.New("invalid token")
}

func (m *JWTMiddleware) Handle(next http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		// TODO generate middleware implement function, delete after code implementation
		// JWTAuthMiddleware Implementation
    // 获取Authorization部分数据
		authHeader := r.Header.Get("Authorization")
		if authHeader == "" { // 判断请求是否为空
			w.WriteHeader(http.StatusBadRequest)
			err, _ := json.Marshal(errorx.NewCodeError(uint32(errorx.ErrHeaderNil), "请求头中auth为空"))
			w.Write(err)
			return
		}
		parts := strings.Split(authHeader, " ") // 获取parts
		if !(len(parts) == 3 && parts[0] == "Bearer") {
			w.WriteHeader(http.StatusBadRequest)
			err, _ := json.Marshal(errorx.NewCodeError(uint32(errorx.ErrHeaderFormat), "请求头格式错误"))
			w.Write(err)
			return
		}
		// 解析token
		claims, isExpire, err := utils.ParseToken(parts[1], parts[2])
		if err != nil {
			w.WriteHeader(http.StatusUnauthorized)
			err, _ := json.Marshal(errorx.NewCodeError(uint32(errorx.ErrTokenProve), "token验证错误"))
			w.Write(err)
			return
		}
		if isExpire {
			parts[1], parts[2] = utils.GetToken(claims.ID, claims.State)
			w.Header().Set("Authorization", fmt.Sprintf("Bearer %s %s", parts[1], parts[2]))
		}
		r = r.WithContext(context.WithValue(r.Context(), UserIDKey, claims.ID))
		// Passthrough to next handler if need
		next(w, r)
	}
}

```

7. 第三方系统如QQ接入后，如何构建JWT？

QQ扫码登录采用的是OAuth2.0协议，接入QQ即通过QQ获取用户信息，避免输入用户名和密码。然后根据QQ获取的用户信息进行用户鉴权和管理。

> **OAuth登录的流程**
> 
> OAuth在客户端client和服务提供商之间，设置了一个授权层`authorization layer`，客户端不能直接登录服务提供商，只能登录授权层，以此将用户与客户端区分开来。客户端登录授权层使用的令牌token，与用户密码不同，用户在登录时，指定令牌的权限和有效期。
> 
> 客户端登录授权层后，服务提供商根据令牌层的权限范围和有效期，向客户端开放用户存储的资料。
>
> **运行流程**
> 
> 1. 用户请求Resource owner，请求Authorization Request，返回Authorization Grant；
> 2. 用户请求Authorization Server（授权服务器），请求Authorization Grant，返回Access Token；
> 3. 用户根据Access Token访问Resource Server，返回Protected Resource。实际上在这里需要获取的是第三方id，根据第三方id再在user表中查找是否授权。

user表中包含了用户的基本信息，如ID、用户名、昵称等；
user_auth表中存储了用户的认证信息，包括创建时间、更新时间、用户ID、第三方平台的唯一ID`privider_id`和平台类型。

这两张表通过user_id字段建立关系。

如何登录：从user_auth表开始，通过provider和provider_id找到对应的user_id，然后使用这个 user_id 在 user 表中找到用户的详细信息。

8. OAuth2.0登录逻辑（代码讲解）
- 申请appid和appkey
  - appid：应用的唯一id，在OAuth认证过程中，appid的值即为oauth_consumer_key的值；
  - appkey：appid对应的密钥，访问用户资源时验证应用的合法性，在oauth2.0认证中，appkey的值即为oauth_consumer_secret的值。
- 授权流程
  - 采用OAuth2.0协议，QQ登录采用的授权类型为`authorization_code`，即**授权码**模式。
  - 用户访问第三方应用，qq返回一个授权的code；
  - 通过授权的code向qq服务器获取token，根据token即可访问具体的api接口。

**代码详解**

- 获取Code：自动打开网页，可以点击QQ号进行登录，或者扫码登录。
```go
func GetAuthCode(w http.ResponseWriter, r *http.Request) {
    params := url.Values{}
    params.Add("response_type", "code")
    params.Add("client_id", AppId)  // 将AppID传入URL
    params.Add("state", "test")
    str := fmt.Sprintf("%s&redirect_uri=%s", params.Encode(), redirectURI)
    loginURL := fmt.Sprintf("%s?%s", "https://graph.qq.com/oauth2.0/authorize", str)

    http.Redirect(w, r, loginURL, http.StatusFound) // 重定向
}
```

- 获取AccessToken：点击QQ登录后，会**回调到后台地址**，回调地址的URL中会带有code参数，根据code可以获取Access Token。
```go
func GetToken(w http.ResponseWriter, r *http.Request) {
    code := r.FormValue("code") // 获取code参数
    params := url.Values{}
    params.Add("grant_type", "authorization_code")  // 授权码验证
    params.Add("client_id", AppId)  // 将AppID传入URL
    params.Add("client_secret", AppKey)  // 将AppKey传入URL
    params.Add("code", code)  // 传入code
    str := fmt.Sprintf("%s&redirect_uri=%s", params.Encode(), redirectURI)
    loginURL := fmt.Sprintf("%s?%s", "https://graph.qq.com/oauth2.0/token", str)
    response, err := http.Get(loginURL)
    if err != nil {
    w.Write([]byte(err.Error()))
    }
    defer response.Body.Close()
    bs, _ := ioutil.ReadAll(response.Body)
    body := string(bs)
    resultMap := convertToMap(body)
    info := &PrivateInfo{}  // 获取返回内容
    info.AccessToken = resultMap["access_token"]
    info.RefreshToken = resultMap["refresh_token"]
    info.ExpiresIn = resultMap["expires_in"]
    GetOpenId(info, w)
}
```

- 获取openId：openId是每个具体用户在我们平台下的唯一标识，所有请求都会带上这个openid
```go
func GetOpenId(info *PrivateInfo, w http.ResponseWriter) {
    resp, err := http.Get(fmt.Sprintf("%s?access_token=%s", "https://graph.qq.com/oauth2.0/me", info.AccessToken))
    if err != nil {
    w.Write([]byte(err.Error()))
    }
    defer resp.Body.Close()
    bs, _ := ioutil.ReadAll(resp.Body)
    body := string(bs)
    info.OpenId = body[45:77]
    GetUserInfo(info, w)
}
```

- 获取用户信息：根据access_token和openid获取用户信息（在不访问资源的时候，这一步可以省略）
```go
func GetUserInfo(info *PrivateInfo, w http.ResponseWriter) {
  params := url.Values{}
  params.Add("access_token", info.AccessToken)
  params.Add("openid", info.OpenId)
  params.Add("oauth_consumer_key", AppId)
  uri := fmt.Sprintf("https://graph.qq.com/user/get_user_info?%s", params.Encode())
  resp, err := http.Get(uri)
  if err != nil {
  w.Write([]byte(err.Error()))
  }
  defer resp.Body.Close()
  bs, _ := ioutil.ReadAll(resp.Body)
  w.Write(bs)
}
```

- 注册应用：首先，您需要在第三方服务提供者（如QQ）的开发者平台上注册您的应用，以获取appid（应用程序ID）和appkey（应用程序密钥）。
- 构造重定向URL：用户访问您的应用时，您的应用会构造一个重定向URL，将用户引导到第三方服务提供者的登录页面。这个URL通常包含appid和重定向URI（redirect_uri），用户在第三方登录页面成功登录后，会被重定向回您的应用，并携带一个授权码（code）。
- 获取授权码：用户在第三方登录页面登录后，如果同意授权，第三方服务提供者会将用户重定向回您的应用，并在URL中附加授权码（code）。
- 交换授权码获取访问令牌：您的应用后端使用授权码、appid、appkey以及redirect_uri向第三方服务提供者的认证服务器发送请求，以交换访问令牌（access_token）和刷新令牌（refresh_token）。
- 获取用户标识：一旦您获得了访问令牌，您可以使用它来请求用户的身份信息，通常是用户的唯一标识符（如openid）。
- 使用访问令牌访问资源：有了访问令牌和用户标识符，您的应用就可以访问第三方服务提供者授权的资源了。


## 分片上传和断点续传
> **sha1值进行文件校验**
> sha1值是一种hash加密函数，常用于数据完整性验证和身份验证。唯一性、不可逆性、一致性、速度快。


### 基于TencentCOS
**文件下载**
- 创建本机缓存文件夹
- 调用COS进行下载
- 根据sha1值进行文件校验
- 设置HTTP响应头，设置Content-Disposition头和Content-Type头，提示用户下载文件
- 删除本地文件，发送文件内容给客户端

**调用COS下载流程**
```go
建立连接 设置超时时间 开启断点续传 查看API能够完成
```

**文件上传**

### 基于Minio集群
**文件下载**
- 分块下载
  - 根据FileName获取对象信息
  - 计算分块数量（`totalParts := int((info.Size + chunkSize - 1) / chunkSize)`）
  - 下载并合并文件
  - 打开合并文件并验证
  - 设置响应头并将文件内容发送给客户端

**分块下载逻辑**
- 创建channel，并发下载
```go
ch := make(chan string, totalParts)
var wg sync.WaitGroup // 并发下载
// 开启协程
for partNum := 0; partNum < totalParts; partNum++ {
    wg.Add(1)
    go l.downloadFilePart(client, bucket, object, partNum, &wg, ch)
}

// 等待所有分块下载完成
wg.Wait()

// 检查是否下载成功
close(ch)
for filePath := range ch { // 关闭之后只是防止新的值被发送到channel，已发送尚未被消费的channel依然可以被接收
  if filePath == "" {
    return fmt.Errorf("下载分块失败")
  }
}
```

- 分块下载具体逻辑
```go
// 合并分块为完成文件
func (l *DownloadMinioLogic) downloadFilePart(client *minio.Client, bucket, object string, partNumber int, wg *sync.WaitGroup, ch chan<- string) {
  defer wg.Done()
  // 创建文件保存下载的分块
  filePath := outputDir + fmt.Sprintf("part%d", partNumber)
	// 分块下载配置
	opts := minio.GetObjectOptions{}
	opts.PartNumber = partNumber

  // 下载分块
  err := client.FGetObject(context.Background(), bucket, object, filePath, opts)
  if err != nil {
    // 异常处理
    logc.Errorf(l.ctx, "下载分块失败: %v", err)
		ch <- "" // 发送空串到通道表示下载失败
		return  
  }
  ch <- filePath // 发送分块下载的文件路径到通道
}
```

- 保存合并后的文件
```go
func (l *DownloadMinioLogic) mergeFileParts(outputDir, outputFileName string, totalParts int) error {
  // 创建输出文件
  outputPath := filepath.join(outputDir, outputFileName)
  outputFile, err := os.Create(outputPath)
  defer outputFile.close()

  // 将分块内容合并到输出文件中
  for i := 0; i < totalParts; i++ {
    partPath := outputDir + fmt.Sprintf("part%d", i)
    partData, err := os.ReadFile(partPath)  // 读取partData
    _, err = outputFile.Write(partData) // 写partData
    // 删除已合并的文件
    os.Remove(partPath)
  }
}
```


**文件上传（逻辑写在rpc中）**
- 文件秒传：rpc调用，传入文件filesha1和userid，根据filesha1判断是否已存在。
```go
l.svcCtx.MysqlDb.Model(&model.File{}).Where("file_sha1 = ?", in.Filesha1).First(&file).Error; fasterr == nil {
  // 文件已存在，触发秒传
  // 构建newUserFile
  newUserFile := &model.UserFile {}
  // 查UserFile表
  l.svcCtx.MysqlDb.Create(&newUserFile)
}
```

- 分块上传
- 初始化：获取userid，文件大小，文件filesha1，rpc调用初始化。其中，分块大小为100*1024*1024，即100M。
  ```go
  // 生成分块上传的初始化信息
  uploadInfo : model.MultiPartUploadInfo{
    FileSha1:   in.FileSha1,
		FileSize:   in.FileSize,
		UploadID:   strconv.FormatInt(in.UserId, 10) + fmt.Sprintf("%x", time.Now().UnixNano()), // 根据userid+时间戳构建上传id
		ChunkSize:  100 * 1024 * 1024,
		ChunkCount: int(math.Ceil(float64(in.FileSize) / (100 * 1024 * 1024))),
  }

  // 将分块信息写入Redis
  // 写入filesha1
  if err := l.svcCtx.RedisClient.HSet(l.ctx, CacheMultipartUploadKey+uploadInfo.UploadID, "filesha1_", uploadInfo.FileSha1).Err(); err != nil {
		return nil, errors.Wrapf(errorx.NewDefaultError("redis写入错误"), "redis写入错误 err:%v", err)
	}
  // 写入filesize
	if err := l.svcCtx.RedisClient.HSet(l.ctx, CacheMultipartUploadKey+uploadInfo.UploadID, "filesize_", uploadInfo.FileSize).Err(); err != nil {
		return nil, errors.Wrapf(errorx.NewDefaultError("redis写入错误"), "redis写入错误 err:%v", err)
	}
  // 写入chunkcount
	if err := l.svcCtx.RedisClient.HSet(l.ctx, CacheMultipartUploadKey+uploadInfo.UploadID, "chunkcount_", uploadInfo.ChunkCount).Err(); err != nil {
		return nil, errors.Wrapf(errorx.NewDefaultError("redis写入错误"), "redis写入错误 err:%v", err)
	}
  ```

- 分块上传逻辑
  - api：获取Redis中分片值，如果为1，表示已经上传。如果部位1，则获取文件内容，创建缓冲区，调用rpc更新文件状态，完成上传。
  - rpc：将HSet中对应分块的值置为1，表示已经上传成功。

创建本地文件夹和文件，缓存分片内容。
```go
// api逻辑
buf := make([]byte, 1024*1024)
for {
  // 从请求的body中读取数据到缓冲区
  n, err := r.Body.Read(buf)  // 每次读取1MB数据，写入文件
  fd.Write(buf[:n])
  // 读完退出循环
  if err != nil {
    break
  }
}
```
> 为什么要创建缓冲区上传
> 

- 文件上传逻辑
1. 获取user_id
2. 接收文件流并存储到本地目录
3. 计算文件sha1值
```go
file.Seek(0, 0)
fileSha1 := utils.FileSha1(file)
file.Seek(0, 0)
```
4. 创建本地目录存储
5. 写入文件`io.Copy`
6. 使用kafka对mysql存储的文件元信息进行**异步处理**
7. 调用rpc进行上传

这里文件上传的主逻辑实现到将用户存储的文件放入服务器这一步。后续再进行文件处理。

**进行Upload的逻辑**
- COSUpload：调用TencentCOSUpload Api进行上传
- Minio：上传到Minio

在这里进行file元数据的异步处理，使用kafka
```go
err = l.batcher.Add(strconv.FormatInt(in.UserId, 10), &userfile)
```

**处理所有上传完毕的分块并合并**

在api主逻辑中实现
1. 通过uploadId查询Redis，判断是否所有分块均上传完成
2. 合并本地文件，形成一个大文件
3. 删除本地文件
4. 调用rpc将合并文件上传到对应存储
```go
result, err := l.svcCtx.RedisClient.HGetAll(l.ctx, CacheMultipartUploadKey+req.UploadID).Result()
count := 0
for k, v := range result {
  // 记录以 checkindex_ 开头且 v 为 1 的 记录的数量
  if strings.HasPrefix(k, CacheMultipartUploadIndex) && v == "1" {
			count++
	}
}

// 所需分片数量不等于从redis查出来的已完成的分片数量 无法满足合并条件

// 合并分块
// 依次读取每个分块文件，并加入到合并文件中
// 读取每个分块文件数据并加入到合并文件中
	for i := 0; i < int(req.ChunkCount); i++ {
		chunkFilePath := l.svcCtx.Config.FileLocalPath + req.UploadID + "/" + strconv.Itoa(i)
		chunkData, err := os.ReadFile(chunkFilePath)
		if err != nil {
			return errors.Wrapf(errorx.NewDefaultError(err.Error()), "os.ReadFile分块文件路径 err:%v", err)
		}

		_, err = mergedFile.Write(chunkData)
		if err != nil {
			return errors.Wrapf(errorx.NewDefaultError(err.Error()), "write分片文件内容到合并文件 err:%v", err)
		}

		// 删除已合并的分块文件
		err = os.Remove(chunkFilePath)
		if err != nil {
			logc.Error(l.ctx, "无法删除已经合并的分块文件 err:", err)
		}
	}
```

## Kafka+Batcher+Channel高并发数据处理
在uploadFileLogic中，如果同时有大量的请求，使用kafka进行流量削峰
**批量数据聚合**

在UploadFile方法中，每上传一个文件都往Kafka中发送一条消息，如果此时有大量的请求同时上传文件，会导致Kakfa中消息激增，可能会导致消息堆积、消费延迟、数据不一致、服务崩溃等问题。

之前每发送一条消息就会产生一次网络IO和磁盘IO，消息聚合后，比如聚合100条消息后再发送，此时100条消息才会产生一次网络IO和磁盘IO，能够提升吞吐量。这就是小包聚合的思想，也叫batcher或者批量处理。

这种思想随处可见，比如Mysql批量插入数据的时候，通过一条sql语句执行而不是循环一条条插入。

**如何选择聚合策略**

聚合有两个维度，分别是**聚合消息条数**和**时间**。

比如聚合100条消息就往kafka中发送一次，这个条数是可以配置的，如果一直达不到100条消息，可以通过聚合时间进行兜底。如无论目前聚合了多少条消息，只要聚合时间达到1秒，就往kafka中发送一次数据。

**批量聚合的工具Batcher**
```go
type Batcher struct {
  opts options
  Do       func(ctx context.Context, val map[string][]interface{})
  Sharding func(key string) int
  chans    []chan *msg
  wait     sync.WaitGroup
}
```

- Do方法：满足聚合条件后就会执行Do方法，其中val参数为聚合后的数据；
- Sharding方法：对key进行sharding，相同key的消息写入同一个channel中，被同一个goroutine处理；

在merge方法中有两个执行Do方法的条件：
- 聚合的数据条数大于等于设置的条数
- 触发设置的定时器

**如何使用**
- 创建Batcher对象，定义Batcher的Sharding方法和Do方法
- Sharing方法通过userid将同一用户的消息聚合到同一个goroutine中处理
- Do方法将聚合的数据一次性批量发动到Kafka

**降低消息的消费延迟**

基于批量处理的思想，提供了Bathcher工具，主要是针对生产端而言。

在消费到批量数据时，需要串行处理一条条数据，加速消费的两种方案分别是：
- 增加消费者的数量
- 在一个消费者中增加消息处理的并行度

在Kafka中，一个Topic可以配置多个Partition，数据被平均或按照生产者指定的方式写入到多个分区中。
在消费时，Kafka约定一个分区只能被一个消费者消费。

因为如果有多个Consumer同事消费一个分区的数据，在操作这个消费进度时需要加锁，对性能影响较大。

当消费者数量小于分区数量时，可以通过增加消费者的数量来提高消息处理能力，但是当消费者数量大于分区时，继续增加消费者数量就没有意义了。

所以在这里，在一个Consumer中提升消息处理的并行度，即通过多个goroutine并行消费数据。

```go
// 创建service实例
	s := &Service{
		c:        c,
		MysqlDb:  mysqlDb,
		msgsChan: make([]chan *model.NewUserFile, chanCount),
	}

	// 创建 chanCount 个消费者 goroutine
	for i := 0; i < chanCount; i++ {
		ch := make(chan *model.NewUserFile, bufferCount)
		s.msgsChan[i] = ch
		s.waiter.Add(1)
		go s.consume(ch) // 启动协程
	}
```

其中，msgsChan为slice，slice的长度为goroutine的数量。

从Kafka中消费到数据后，将数据投入goroutine中，投递消息时按照userId进行Sharding，确保对同一userId的处理是串行的。

```go
// Consumer 消费者方法用于处理消息
func (s *Service) Consumer(_ string, value string) error {
	logx.Infof("消费消息: %s\n", value)
	// 解析JSON数据为 []*model.NewUserFile 对象
	var data []*model.NewUserFile
	if err := json.Unmarshal([]byte(value), &data); err != nil {
		return err
	}

	// 将解析后的消息根据 UserId 分发到不同的通道
	for _, d := range data {
		s.msgsChan[d.UserId%chanCount] <- d
	}
	return nil
}
```

## Minio是什么
**Minio是什么**

**为什么选择Minio**

## 分片上传和断点上传的区别

**分片上传**

将一个大文件分成多个较小的片段进行并行上传，最终在服务端合并这些片段形成完整文件。
- 分片处理：文件被分为多个小片段，在客户端完成分片；
- 并行上传：提高上传效率和速度；
- 重传片段：如果某个片段上传失败，无需重传整个文件；

适用场景：
- 大文件上传
- 网络不稳定
- 高并发上传

**断点上传**

允许上传过程中在中断后恢复，上传文件时，记录当前上传的位置，当上传中断时，能够从中断的位置继续上传，而不必重新开始。
- 位置记录：上传过程中记录已上传的部分，可以在中断后继续；
- 断点续传：上传终端后，可以从中断的地方继续上传；
- 状态保存：需要在本地和服务端保存上传进度信息；

适用场景：
- 大文件上传
- 长时间上传
- 低带宽网络

断点上传：HTTP Range请求

```go
// 客户端代码
// 打开文件
// 计算file大小
// 记录uploadSize
// 发送head请求，head请求不会获取响应体，只会获取头部信息，主要用户检查资源信息如大小、是否存在等
// 检查响应码是否为200,表示服务器确认的head请求且资源存在;从响应头中获取Content-Range头信息
// 检查已上传的文件大小 contentRange := resp.Header.Get("Content-Range")
// 如果content-range不为空,拆分字符串,获取已经上传的字节数,转换为整数
// 从upload size开始上传
// 使用http.put上传文件
// 通过服务端更新upload size


// 服务端代码
// 处理上传请求，打开或创建目标文件
// 获取请求头中的Content-Range信息，确定要写入的位置
// 使用io.Copy将上传的数据写入文件，并返回适当的HTTP响应码
```

Content-Range头格式

Content-Range: bytes start-end/total

**什么是HTTP Range**



**断点上传和分片上传集成**
